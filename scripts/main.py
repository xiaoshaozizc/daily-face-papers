#!/usr/bin/env python3
"""
æ¯æ—¥è®ºæ–‡æ¨èä¸»è„šæœ¬
æ•´åˆå¤šæ¥æºè®ºæ–‡è·å–ï¼Œè‡ªåŠ¨ç”Ÿæˆæ¨è
"""

import os
import sys
import json
from datetime import datetime

# æ·»åŠ è„šæœ¬ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from fetcher import PaperAggregator
from generator import ReadmeGenerator


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("  æ¯æ—¥äººè„¸è¯†åˆ«/ç”Ÿæˆè®ºæ–‡æ¨èç³»ç»Ÿ (å¤šæ¥æºç‰ˆ)")
    print(f"  è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    # è·å–è®ºæ–‡ (ä»å¤šä¸ªæ¥æº)
    print("\n[1/3] æ­£åœ¨ä»å¤šä¸ªæ¥æºè·å–è®ºæ–‡...")
    aggregator = PaperAggregator()
    papers = aggregator.fetch_all(days=30)

    print(f"\n  ğŸ“Š ç»Ÿè®¡:")
    print(f"     - äººè„¸è¯†åˆ«è®ºæ–‡: {len(papers['face_recognition'])} ç¯‡")
    print(f"     - äººè„¸ç”Ÿæˆè®ºæ–‡: {len(papers['face_generation'])} ç¯‡")
    print(f"     - æ€»è®¡: {len(papers['all'])} ç¯‡")
    print(f"\n  ğŸ“¡ æ¥æºç»Ÿè®¡:")
    for source, count in papers.get('sources', {}).items():
        print(f"     - {source}: {count} ç¯‡")

    if not papers["all"]:
        print("\nâš ï¸ æš‚æ— æ–°è®ºæ–‡")
        return

    # ç”Ÿæˆ README
    print("\n[2/3] æ­£åœ¨ç”Ÿæˆ README...")
    generator = ReadmeGenerator()

    # è·å–é¡¹ç›®æ ¹ç›®å½•
    repo_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    readme_path = os.path.join(repo_root, "README.md")

    readme_content = generator.generate_readme(papers)
    generator.save_readme(readme_content, readme_path)

    # ä¿å­˜è®ºæ–‡æ•°æ®
    print("\n[3/3] æ­£åœ¨ä¿å­˜è®ºæ–‡æ•°æ®...")
    data_path = os.path.join(repo_root, "papers.json")
    generator.save_papers_json(papers, data_path)

    # ä¿å­˜æ¯æ—¥å­˜æ¡£
    today = datetime.now().strftime("%Y-%m-%d")
    papers_dir = os.path.join(repo_root, "papers")
    os.makedirs(papers_dir, exist_ok=True)
    daily_path = os.path.join(papers_dir, f"{today}.md")

    # ç”Ÿæˆæ¯æ—¥å­˜æ¡£
    daily_content = f"# {today} è®ºæ–‡æ¨è\n\n"
    daily_content += f"**æ¥æº**: arXiv, CVPR, NeurIPS ç­‰\n\n"
    daily_content += f"**æ€»è®¡**: {len(papers['all'])} ç¯‡ (äººè„¸è¯†åˆ«: {len(papers['face_recognition'])}, äººè„¸ç”Ÿæˆ: {len(papers['face_generation'])})\n\n---\n\n"

    for i, paper in enumerate(papers["all"], 1):
        daily_content += f"## {i}. {paper['title']}\n\n"
        daily_content += f"- **æ¥æº**: {paper.get('source', 'Unknown')}\n"
        daily_content += f"- **åˆ†ç±»**: {paper.get('category')}\n"
        daily_content += f"- **ä½œè€…**: {', '.join(paper['authors']) if paper['authors'] else 'Unknown'}\n"
        daily_content += f"- **æ—¥æœŸ**: {paper['published']}\n"
        if paper.get('arxiv_link'):
            daily_content += f"- **é“¾æ¥**: {paper['arxiv_link']}\n"
        if paper.get('pdf_link'):
            daily_content += f"- **PDF**: {paper['pdf_link']}\n"
        daily_content += f"\n**ç®€ä»‹**: {paper['summary']}\n\n---\n\n"

    with open(daily_path, "w", encoding="utf-8") as f:
        f.write(daily_content)
    print(f"  æ¯æ—¥å­˜æ¡£å·²ä¿å­˜åˆ°: {daily_path}")

    print("\n" + "=" * 60)
    print("  âœ… å®Œæˆ! è®ºæ–‡æ¨èå·²ç”Ÿæˆ")
    print("=" * 60)


if __name__ == "__main__":
    main()
